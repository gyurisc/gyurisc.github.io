
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Scraping Stackoverflow Careers for Fun and Profit - Part 2 - littlebigtomatoes</title>
  <meta name="author" content="Krisztian Gyuris">

  
  <meta name="description" content="Let’s continue with our project. To summarise what we did in the first part, we wrote a scraper in python using the scrapy framework that was capable &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://www.littlebigtomatoes.com/2016/08/scraping-stackoverflow-careers-for-fun-and-profit-part-2">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="http://feeds.feedburner.com/Littlebigtomatoes" rel="alternate" title="littlebigtomatoes" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!-- scripts -->
<script src="//load.sumome.com/" data-sumo-site-id="d6a3c7c27daf3bb0caf1bf90258abbef17481531daec62e88a3c255080954dcf" async="async"></script>
<!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-55456-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">littlebigtomatoes</a></h1>
  
    <h2>Design.Create.Smile.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="http://feeds.feedburner.com/Littlebigtomatoes" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:www.littlebigtomatoes.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/about">About</a></li>
  <li><a href="/projects">Projects</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Scraping Stackoverflow Careers for Fun and Profit - Part 2</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-08-06T21:20:17+02:00'><span class='date'><span class='date-month'>Aug</span> <span class='date-day'>6</span><span class='date-suffix'>th</span>, <span class='date-year'>2016</span></span> <span class='time'>9:20 pm</span></time>
        
           | <a href="#disqus_thread"
             data-disqus-identifier="http://www.littlebigtomatoes.com">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Let’s continue with our project. To summarise what we did in the <a href="http://littlebigtomatoes.com/2016/07/scraping-stackoverflow-careers-for-fun-and-profit/">first part</a>, we wrote a scraper in python using the <a href="http://scrapy.org">scrapy framework</a> that was capable of getting data from the stackoverflow job pages, but nothing else than that.</p>

<p>In this part, we will save the results to mongodb, make sure that we do not have duplicates and learn how to export the data we gathered&hellip;</p>

<!--more-->


<p></p>

<p>The source code for this project can be found in <a href="https://github.com/gyurisc/stackjobs">stackjobs repo</a> on github.</p>

<h3>Installing mongodb</h3>

<p>I need to store the data somewhere and I choose to use mongodb, because of no particular reason :). Let&rsquo;s install it via the <a href="http://brew.sh/">homebrew</a> package manager by using the commands:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>brew update
</span><span class='line'>brew install mongodb
</span></code></pre></td></tr></table></div></figure>


<p>If you want to download the install package or use some other methods to install, please visit the page <a href="https://docs.mongodb.com/master/tutorial/install-mongodb-on-os-x/">Install MongoDB Community Edition on OS X</a> and choose the method that suits you.</p>

<p>Then create the directory structure for mongodb in a suitable location and start the server</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>mkdir mongodb/data/db
</span><span class='line'>mongod --rest --dbpath mongodb/
</span></code></pre></td></tr></table></div></figure>


<p>You can confirm that the mongodb service is up and running by visiting <a href="http://localhost:28017">http://localhost:28017</a>.</p>

<h4>How to configure mongo to run as a service?</h4>

<p>Because, I installed mongodb using brew I can also start, stop and list mongodb as a service on macOS with the following command:</p>

<figure class='code'><figcaption><span>Starting and stopping mongodb servce using brew.</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>brew services start mongodb
</span><span class='line'>brew services stop mongodb
</span><span class='line'>brew services list
</span></code></pre></td></tr></table></div></figure>


<p>My only problem is with this approach is that I do not know how can I enable the &ndash;rest interface in this case, but navigating to <a href="http://localhost:27017">http://localhost:27017</a> I get the standard mongodb warning, meaning that my database engine is running and that is good enough for now.</p>

<p>In order, to communicate with mongodb in python, you will need to install pymongo module using pip:</p>

<figure class='code'><figcaption><span>Installing pymongo</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pip install pymongo
</span></code></pre></td></tr></table></div></figure>


<h3>Saving the items in db</h3>

<p>To be able to save the items gathered you will need to add some configuration to your project and also add some code to your pipelines.py.</p>

<h4>Setting up a Pipeline</h4>

<p>First, let’s see how the pipeline works. We will define a new class called MongoDBPipeline. This class has a constructor and a process_item method that will be called whenever a new item is created.</p>

<p>We will be using the <strong>process_item</strong> to check the validity of the item first, then it will be checked against a list called <strong>ids_seen</strong>. This list contains all the items that we already processed. We will ensure this way that there will be  no duplicated items in our database.</p>

<p>If the item is valid and it is not in our list of previous items then we ca add it to our mongo database.</p>

<figure class='code'><figcaption><span>Validating and adding the item to our mongodb</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">valid</span> <span class="o">=</span> <span class="bp">True</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">item</span><span class="p">:</span>
</span><span class='line'>        <span class="k">if</span> <span class="ow">not</span> <span class="n">data</span><span class="p">:</span>
</span><span class='line'>            <span class="n">valid</span> <span class="o">=</span> <span class="bp">False</span>
</span><span class='line'>            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s">&quot;Missing {0}!&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">if</span> <span class="n">valid</span> <span class="ow">and</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;jobid&#39;</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span><span class="p">:</span>
</span><span class='line'>        <span class="bp">self</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">))</span>
</span><span class='line'>        <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s">&#39;jobid&#39;</span><span class="p">])</span>
</span><span class='line'>        <span class="n">log</span><span class="o">.</span><span class="n">msg</span><span class="p">(</span><span class="s">&quot;Job added to MongoDB database!&quot;</span><span class="p">,</span>
</span><span class='line'>                <span class="n">level</span><span class="o">=</span><span class="n">log</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">,</span> <span class="n">spider</span><span class="o">=</span><span class="n">spider</span><span class="p">)</span>
</span><span class='line'>    <span class="k">else</span><span class="p">:</span>
</span><span class='line'>        <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s">&quot;Job id {0} already exists!&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s">&#39;jobid&#39;</span><span class="p">]))</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">return</span> <span class="n">item</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now, we need to check out the constructor for this class. Here we initialise the connection and fill the <strong>ids_seen</strong> list with the identifiers that we already have in the database.</p>

<figure class='code'><figcaption><span>Initialising database connection and fill up our id list.</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">pymongo</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">sys</span>
</span><span class='line'>
</span><span class='line'><span class="kn">from</span> <span class="nn">scrapy.conf</span> <span class="kn">import</span> <span class="n">settings</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">log</span>
</span><span class='line'>
</span><span class='line'><span class="k">class</span> <span class="nc">MongoDBPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span><span class='line'>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class='line'>        <span class="n">connection</span> <span class="o">=</span> <span class="n">pymongo</span><span class="o">.</span><span class="n">MongoClient</span><span class="p">(</span>
</span><span class='line'>            <span class="n">settings</span><span class="p">[</span><span class="s">&#39;MONGODB_SERVER&#39;</span><span class="p">],</span>
</span><span class='line'>            <span class="n">settings</span><span class="p">[</span><span class="s">&#39;MONGODB_PORT&#39;</span><span class="p">]</span>
</span><span class='line'>        <span class="p">)</span>
</span><span class='line'>        <span class="n">db</span> <span class="o">=</span> <span class="n">connection</span><span class="p">[</span><span class="n">settings</span><span class="p">[</span><span class="s">&#39;MONGODB_DB&#39;</span><span class="p">]]</span>
</span><span class='line'>        <span class="bp">self</span><span class="o">.</span><span class="n">collection</span> <span class="o">=</span> <span class="n">db</span><span class="p">[</span><span class="n">settings</span><span class="p">[</span><span class="s">&#39;MONGODB_COLLECTION&#39;</span><span class="p">]]</span>
</span><span class='line'>        <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span><span class='line'>        <span class="k">for</span> <span class="n">job</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">find</span><span class="p">():</span>
</span><span class='line'>            <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">job</span><span class="p">[</span><span class="s">&#39;jobid&#39;</span><span class="p">])</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">print</span> <span class="s">&quot;Number of jobs in database&quot;</span>
</span><span class='line'>        <span class="k">print</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<h4>Configuring the mongodb connection.</h4>

<p>We need to do a little more work in order to be able to connect to the database and to register the newly added MongoDBPipeline, so it is actually called on the new items. In the settings.py, we need to added either add or modify the following line, so it looks like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span><span class="s">&#39;stackjobs.pipelines.MongoDBPipeline&#39;</span> <span class="p">:</span> <span class="mi">300</span><span class="p">,</span> <span class="p">}</span>
</span><span class='line'>
</span><span class='line'><span class="n">MONGODB_SERVER</span> <span class="o">=</span> <span class="s">&quot;localhost&quot;</span>
</span><span class='line'><span class="n">MONGODB_PORT</span> <span class="o">=</span> <span class="mi">27017</span>
</span><span class='line'><span class="n">MONGODB_DB</span> <span class="o">=</span> <span class="s">&quot;stackoverflow&quot;</span>
</span><span class='line'><span class="n">MONGODB_COLLECTION</span> <span class="o">=</span> <span class="s">&quot;jobs&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>With the first line, we register our pipeline class on the system and assign a priority. With this mechanism, we can specify    in what we order we want the pipelines to be executed, if we would have more than one pipelines.</p>

<p>The four lines after that are specifying how to connect to the database, such as the name of the server and server port, database name and collection name (table name).</p>

<p>With these changes we can now run our parser and all items scraped will be stored in our mongo database.</p>

<h3>Exporting data from mongo</h3>

<p>What to do next? Maybe, we want to export the data to a file, so we can do further analysis and cleaning up on it. For this, I used the mongoexport command with the following parameters to create a csv file</p>

<figure class='code'><figcaption><span>Exporting data to csv file.</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>mongoexport --db stackoverflow --collection <span class="nb">jobs</span> --type csv --fields jobid,title,employer,location,salary,description,tags,url,date --out stackoverflow_jobs.csv
</span></code></pre></td></tr></table></div></figure>


<p>It is also possible to export the data in son format, if needed.  For that, you can use the following command:</p>

<figure class='code'><figcaption><span>Exporting data in json format. </span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>mongoexport --db stackoverflow --collection <span class="nb">jobs</span> --type json --fields jobid,title,employer,location,salary,description,tags,url,date --out stackoverflow_jobs.json
</span></code></pre></td></tr></table></div></figure>


<h3>Scheduling our job</h3>

<p>The last thing we need to do is to schedule our scraper to run on my mac. This can be achieved using crontab. I will schedule my scraper to run in every 6 hours as I do not think that running it more often would make sense.</p>

<p>First, you will need to start the nano editor to edit your crontab file:</p>

<figure class='code'><figcaption><span>Launching nano to edit crontab.</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>env <span class="nv">EDITOR</span><span class="o">=</span>nano crontab -e
</span></code></pre></td></tr></table></div></figure>


<p>Now enter the following to run our scraper every 6 hours. Use Ctrl-0 and Ctrl-X to exit the editor and save.</p>

<figure class='code'><figcaption><span>Enter this line to your crontab file. </span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="m">0</span> */6 * * *  <span class="nb">cd</span> ~/dev/scrapers/stackjobs <span class="o">&amp;&amp;</span> /usr/local/bin/scrapy crawl Stackjob
</span></code></pre></td></tr></table></div></figure>


<p>To list the existing crontab jobs, we can use the following command:</p>

<figure class='code'><figcaption><span>Listing existing crontab jobs </span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>crontab -l
</span></code></pre></td></tr></table></div></figure>


<p></p>

<h3>Summary</h3>

<p>In this part, we have installed mongodb on our machine and added logic and configuration to the scrapy project, so we can save all items that has been found to the database. Also, We have added a way to export the data into CSV or JSON file format from the mongo database. Finally, added this script to the crontab job and scheduled to run it every 2 hours.</p>

<p>In the next part, I will spend a little time on the exported data and use pandas to transform the exported data into a a better consumable format.</p>

<h3>Additional Information:</h3>

<ul>
<li><a href="https://ole.michelsen.dk/blog/schedule-jobs-with-crontab-on-mac-osx.html">Schedule jobs with crontab on Mac OS X</a></li>
<li><a href="http://alvinalexander.com/linux/unix-linux-crontab-every-minute-hour-day-syntax">Unix &amp; Linux crontab every examples</a></li>
</ul>

</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Krisztian Gyuris</span></span>

      




<time class='entry-date' datetime='2016-08-06T21:20:17+02:00'><span class='date'><span class='date-month'>Aug</span> <span class='date-day'>6</span><span class='date-suffix'>th</span>, <span class='date-year'>2016</span></span> <span class='time'>9:20 pm</span></time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/mongodb/'>mongodb</a>, <a class='category' href='/blog/categories/python/'>python</a>, <a class='category' href='/blog/categories/scrapy/'>scrapy</a>, <a class='category' href='/blog/categories/stackoverflow/'>stackoverflow</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://www.littlebigtomatoes.com/2016/08/scraping-stackoverflow-careers-for-fun-and-profit-part-2/" data-via="gyurisc" data-counturl="http://www.littlebigtomatoes.com/2016/08/scraping-stackoverflow-careers-for-fun-and-profit-part-2/" >Tweet</a>
  
  
  
    <div class="fb-like" data-send="true" data-width="450" data-show-faces="false"></div>
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/2016/07/scraping-stackoverflow-careers-for-fun-and-profit/" title="Previous Post: Scraping stackoverflow careers for fun and profit">&laquo; Scraping stackoverflow careers for fun and profit</a>
      
      
        <a class="basic-alignment right" href="/2016/08/getting-started-with-handsontable/" title="Next Post: Getting Started with Handsontable">Getting Started with Handsontable &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/2017/02/disabling-captive-portal-check-in-android/">Disabling Captive Portal Check in Android</a>
      </li>
    
      <li class="post">
        <a href="/2016/09/playing-with-pandas/">Playing With Pandas</a>
      </li>
    
      <li class="post">
        <a href="/2016/08/getting-started-with-handsontable/">Getting Started With Handsontable</a>
      </li>
    
      <li class="post">
        <a href="/2016/08/scraping-stackoverflow-careers-for-fun-and-profit-part-2/">Scraping Stackoverflow Careers for Fun and Profit - Part 2</a>
      </li>
    
      <li class="post">
        <a href="/2016/07/scraping-stackoverflow-careers-for-fun-and-profit/">Scraping Stackoverflow Careers for Fun and Profit</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/gyurisc">@gyurisc</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'gyurisc',
            count: 4,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>


<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  <script type="text/javascript">
    $(document).ready(function(){
      getTwitterFeed("gyurisc", 4, false);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/gyurisc" class="twitter-follow-button" data-show-count="false">Follow @gyurisc</a>
  
</section>


<section>
  <h1>My Pinboard</h1>
  <ul id="pinboard_linkroll">Fetching linkroll&#8230;</ul>
  <p><a href="http://pinboard.in/u:gyurisc">My Pinboard Bookmarks &raquo;</a></p>
</section>
<script type="text/javascript">
  var linkroll = 'pinboard_linkroll'; //id target for pinboard list
  var pinboard_user = "gyurisc"; //id target for pinboard list
  var pinboard_count = 3; //id target for pinboard list
  (function(){
    var pinboardInit = document.createElement('script');
    pinboardInit.type = 'text/javascript';
    pinboardInit.async = true;
    pinboardInit.src = '/javascripts/pinboard.js';
    document.getElementsByTagName('head')[0].appendChild(pinboardInit);
  })();
</script>




  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - Krisztian Gyuris -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'littlebigtomatoes';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://www.littlebigtomatoes.com/2016/08/scraping-stackoverflow-careers-for-fun-and-profit-part-2/';
        var disqus_url = 'http://www.littlebigtomatoes.com/2016/08/scraping-stackoverflow-careers-for-fun-and-profit-part-2/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>





  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
