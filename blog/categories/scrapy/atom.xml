<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Scrapy | littlebigtomatoes]]></title>
  <link href="http://www.littlebigtomatoes.com/blog/categories/scrapy/atom.xml" rel="self"/>
  <link href="http://www.littlebigtomatoes.com/"/>
  <updated>2016-07-14T22:02:56+02:00</updated>
  <id>http://www.littlebigtomatoes.com/</id>
  <author>
    <name><![CDATA[Krisztian Gyuris]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Scraping Stackoverflow Careers for Fun and Profit]]></title>
    <link href="http://www.littlebigtomatoes.com/2016/07/scraping-stackoverflow-careers-for-fun-and-profit/"/>
    <updated>2016-07-12T07:55:13+02:00</updated>
    <id>http://www.littlebigtomatoes.com/2016/07/scraping-stackoverflow-careers-for-fun-and-profit</id>
    <content type="html"><![CDATA[<p>When you want to learn something new the best way to do is to come up with a problem that can be useful to you or maybe to others and then solve it. Not so long ago I decided that I want to start learning something data related. So I came up with the idea that I would create a solution that gathers the data from <a href="http://stackoverflow.com/jobs">Stackoverflow Jobs</a>. Do some manipulation on it  and then present it in a nice format. This is the first step of the journey.</p>

<p>To present something, I need data. To get the data, I need to write a script that gathers data from the web. I decided to use python and <a href="http://scrapy.org">Scrapy framework</a>.</p>

<!--more-->


<p></p>

<h3>Gearing up…</h3>

<p>First, some disclaimers. I am new to python and to scrapy as well. I had zero knowledge any of these topics before. I am learning this stuff as I write these posts. Any constructive and helpful comments are more than welcome.</p>

<p>The code for this project can be found in <a href="https://github.com/gyurisc/stackjobs">stackjobs repo on github</a>.</p>

<p>After installing Python and <a href="http://scrapy.org">Scrapy</a>, the first thing is to create the web-scraper project by typing the following  command(s) in your console:</p>

<pre><code class="bash">        scrapy startproject stackjobs
        cd stackjobs
        scrapy genspider Stackjob stackoverflow.com
</code></pre>

<p>With these commands the following has been achieved. First we created a new project called stackjobs. The script created a folder structure with files in it. Then we entered the folder and created a spider called Stackjob using the genspider command. This created a spider that can crawl stackoverflow.com.</p>

<p>Looking at the spiders folder, we can see the source file for Stackjob spider:</p>

<pre><code class="python spider created ">        # -*- coding: utf-8 -*-
        import scrapy


        class StackjobSpider(scrapy.Spider):
            name = "Stackjob"
            allowed_domains = ["stackoverflow.com"]
            start_urls = (
          'http://www.stackoverflow.com/',
            )

            def parse(self, response):
                pass
</code></pre>

<p>You can run the spider this by executing the following command:</p>

<pre><code class="bash running the spider ">scrapy crawl Stackjob
</code></pre>

<p>This outputs a bunch of stuff that we are not interested in really.</p>

<pre><code class="bash bunch of stuff we are not really interested in">2016-07-11 22:22:30 [scrapy] INFO: Scrapy 1.1.0 started (bot: stackjobs)
2016-07-11 22:22:30 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'stackjobs.spiders', 'SPIDER_MODULES': ['stackjobs.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'stackjobs'}
…
2016-07-11 22:22:30 [scrapy] INFO: Enabled item pipelines:
[]
2016-07-11 22:22:30 [scrapy] INFO: Spider opened
2016-07-11 22:22:30 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-07-11 22:22:30 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2016-07-11 22:22:30 [scrapy] DEBUG: Redirecting (301) to &lt;GET http://stackoverflow.com/robots.txt&gt; from &lt;GET http://www.stackoverflow.com/robots.txt&gt;
2016-07-11 22:22:30 [scrapy] DEBUG: Crawled (200) &lt;GET http://stackoverflow.com/robots.txt&gt; (referer: None)
2016-07-11 22:22:31 [scrapy] DEBUG: Redirecting (301) to &lt;GET http://stackoverflow.com/&gt; from &lt;GET http://www.stackoverflow.com/&gt;
2016-07-11 22:22:31 [scrapy] DEBUG: Crawled (200) &lt;GET http://stackoverflow.com/robots.txt&gt; (referer: None)
2016-07-11 22:22:31 [scrapy] DEBUG: Crawled (200) &lt;GET http://stackoverflow.com/&gt; (referer: None)
2016-07-11 22:22:31 [scrapy] INFO: Closing spider (finished)
2016-07-11 22:22:31 [scrapy] INFO: Spider closed (finished)
</code></pre>

<p>This basically tells us that our spider ran and downloaded zero pages exactly. We are interested in the Stackoverflow job listing, so let’s modify the Stackjob spider by adding start urls that are relevant for us:</p>

<pre><code class="python adding start urls ">class StackjobSpider(scrapy.Spider):
    name = "Stackjob"
    allowed_domains = ["stackoverflow.com"]
    start_urls = (
        'https://stackoverflow.com/jobs?sort=p',
        'https://stackoverflow.com/jobs?sort=p&amp;pg=2',
        'https://stackoverflow.com/jobs?sort=p&amp;pg=3',
        'https://stackoverflow.com/jobs?sort=p&amp;pg=4'
    )

    def parse(self, response):
        pass
</code></pre>

<p>When running the spider again we can find that still nothing really happens. The pages are downloaded, but we do not process the information on the pages. In order to process the information, let’s modify the parse method in our spider.</p>

<p>First we need to add a few imports:</p>

<pre><code class="python">import scrapy
import datetime

class StackjobSpider(scrapy.Spider):
    name = "Stackjob"
    allowed_domains = ["stackoverflow.com"]
    start_urls = (
        'https://stackoverflow.com/jobs?sort=p',
        'https://stackoverflow.com/jobs?sort=p&amp;pg=2',
        'https://stackoverflow.com/jobs?sort=p&amp;pg=3'
    )
</code></pre>

<p>Then add some code to the <em>parse</em> method. This code will loop through all the jobs on the page found by our path expression and will create a <em>StackjobItem</em> for each job found. Then it will add today’s date to the newly created item.</p>

<pre><code class="python adding parsing logic ">    def parse(self, response):
        jobs = scrapy.Selector(response).xpath('//div[contains(@class, "-item") and contains(@class, "-job")]')
        results = [] 

        t = datetime.date.today()

        for job in jobs: 
            item = StackjobItem()  
                        item['date'] = t.strftime('%Y-%m-%d')       

                        # actual parsing will come here 

            results.append(item)

        return results 
</code></pre>

<p>When running this code, you will receive a couple of messages complaining about StackjobItem not being defined. This is expected as we still need to define this class. Open up <em>items.py</em> and rename the class <em>StackjobsItem</em> class to <em>StackjobItem</em>. This is the class what we will use to store the extracted data for each job. For this you will need to define a few fields. The final class will look like this:</p>

<pre><code class="python StackjobItem with fields">
import scrapy
from scrapy.item import Item, Field

class StackjobItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    title = Field()
    url = Field()
    jobid = Field()
    employer = Field()
    description = Field()    
    location = Field()
    equity = Field()
    salary = Field()
    tags = Field()    
    date = Field()
    pass
</code></pre>

<p>Now, head back to <em>Stackjob.py</em> and import the item class.</p>

<pre><code class="python ">from stackjobs.items import StackjobItem
</code></pre>

<p>When running the spider, you should see something like this in the output:</p>

<pre><code class="bash ">2016-07-12 07:07:17 [scrapy] DEBUG: Scraped from &lt;200 https://stackoverflow.com/jobs?sort=p&amp;pg=3&gt;
{'date': '2016-07-12'}
2016-07-12 07:07:17 [scrapy] DEBUG: Scraped from &lt;200 https://stackoverflow.com/jobs?sort=p&amp;pg=3&gt;
{'date': '2016-07-12'}
2016-07-12 07:07:17 [scrapy] DEBUG: Scraped from &lt;200 https://stackoverflow.com/jobs?sort=p&amp;pg=3&gt;
{'date': '2016-07-12'}
2016-07-12 07:07:17 [scrapy] DEBUG: Scraped from &lt;200 https://stackoverflow.com/jobs?sort=p&amp;pg=3&gt;
{'date': '2016-07-12'}
2016-07-12 07:07:17 [scrapy] DEBUG: Scraped from &lt;200 https://stackoverflow.com/jobs?sort=p&amp;pg=3&gt;
{'date': '2016-07-12'}
</code></pre>

<p>Now, this tells us that the items are being generated and each item has one field only and that is the <em>date</em> field. This is not really useful, so let’s add more information.</p>

<h3>To the actual scraping</h3>

<p>For getting information out of the items found on the page, we will be using xpath expressions. Earlier, we added one to our parse method. This selector is responsible to find all the jobs on our page. It basically tells scrapy to return all elements that are a div and has both a <em>-job</em> and <em>-item</em> css class.</p>

<pre><code class="python ">jobs = Selector(response).xpath('//div[contains(@class, "-item") and contains(@class, "-job")]')
</code></pre>

<p>Next, we will enumerate through the collection returned in the <em>jobs</em> variable and extract some more information from each item and store it.</p>

<p>In the <em>parse</em> method of our spider, just below the date field add the following lines:</p>

<pre><code class="python xpaths and css selectors">item['tags'] = job.xpath('div[contains(@class,"tags")]/p/a/text()').extract()
item['description'] = job.xpath('p[contains(@class,"text") and contains(@class, "description")]/text()').extract()[0]
item['location'] = job.xpath('ul[contains(@class, "metadata") and contains(@class, "primary")]/li[contains(@class, "location")]/text()').extract()[0].strip()
item['employer'] = job.xpath('ul[contains(@class, "metadata") and contains(@class, "primary")]/li[contains(@class, "employer")]/text()').extract()[0].strip()
item['jobid'] = job.xpath('@data-jobid').extract()[0]
item['title'] = job.xpath('div[contains(@class,"-title")]/h1/a/text()').extract()[0]
item['url'] = job.xpath('div[contains(@class,"-title")]/h1/a/@href').extract()[0]
</code></pre>

<p>Here, we have an xpath expression for each field that we are interested in. For coming up with the xpath expression I was using Google Chrome and its wonderful <a href="https://www.codeschool.com/courses/discover-devtools">DevTools</a>. You can bring this up by opening a web-page and right-clicking on any element and select <em>Inspect</em> from the menu.</p>

<p>The other stuff we need to come up with the expressions is the knowledge of css and lot’s of trial and error.</p>

<h3>XPath and CSS munging</h3>

<p>The other stuff we need to come up with the expressions is the knowledge of css and lot’s of trial and error. Let’s see a few expressions:</p>

<pre><code class="python title and url ">item['title'] = job.xpath('div[contains(@class,"-title")]/h1/a/text()').extract()[0]
item['url'] = job.xpath('div[contains(@class,"-title")]/h1/a/@href').extract()[0]
</code></pre>

<p>Here, we are searching with a div that has the <em>title</em> as a css class and then we need the link <em>a</em> under the <em>h1</em> heading. The text from this link will be store in the <em>title</em> field and the actual link in the <em>url</em> field.</p>

<pre><code class="python extracting tags">item['tags'] = job.xpath('div[contains(@class,"tags")]/p/a/text()').extract()
</code></pre>

<p>Here, we are looking for a div with a class <em>tags</em> and all the text from the links underneath the paragraph.</p>

<pre><code class="python extracting the employer"> item['employer'] = job.xpath('ul[contains(@class, "metadata") and contains(@class, "primary")]/li[contains(@class, "employer")]/text()').extract()[0].strip()
</code></pre>

<p>Here, we are looking for an unordered list <em>ul</em> with both a class <em>metadata</em> and <em>primary</em> and underneath we are looking for a list item with <em>employer</em> class. We then extract the first item from the returned list and call <em>strip()</em> to get rid of the whitespace characters.</p>

<p>There is one last field that needs to be extracted called <em>salary</em>. This is a bit tricky, because it is not always present.</p>

<p>First, we need to check, if we have an html span tag with class <em>salary</em> our title, if we have then we just extract it and then strip out of the white spaces from the result.</p>

<pre><code class="python checking if salary is present and extracting it ">if job.xpath('div[contains(@class,"-title”)]/span[contains(@class,"salary")]/text()').extract():
    item['salary'] = job.xpath('div[contains(@class,"-title")]/span[contains(@class,"salary")]/text()').extract()[0].strip()
</code></pre>

<p>Here is how the full parse method should look like:</p>

<pre><code class="python final parse method ">
    def parse(self, response):
        jobs = Selector(response).xpath('//div[contains(@class, "-item") and contains(@class, "-job")]')
        results = [] 

        t = datetime.date.today()

        for job in jobs: 
            item = StackjobItem()    
            item['date'] = t.strftime('%Y-%m-%d')                       
            item['tags'] = job.xpath('div[contains(@class,"tags")]/p/a/text()').extract()
            item['description'] = job.xpath('p[contains(@class,"text") and contains(@class, "description")]/text()').extract()[0]
            item['location'] = job.xpath('ul[contains(@class, "metadata") and contains(@class, "primary")]/li[contains(@class, "location")]/text()').extract()[0].strip()
            item['employer'] = job.xpath('ul[contains(@class, "metadata") and contains(@class, "primary")]/li[contains(@class, "employer")]/text()').extract()[0].strip()
            item['jobid'] = job.xpath('@data-jobid').extract()[0]
            item['title'] = job.xpath('div[contains(@class,"-title")]/h1/a/text()').extract()[0]
            item['url'] = job.xpath('div[contains(@class,"-title")]/h1/a/@href').extract()[0]

            if job.xpath('div[contains(@class,"-title")]/span[contains(@class,"salary")]/text()').extract():
                item['salary'] = job.xpath('div[contains(@class,"-title")]/span[contains(@class,"salary")]/text()').extract()[0].strip()

            results.append(item)
            # yield item

        return results 
</code></pre>

<p>After running the parser, we should see the extra fields being added to our item.</p>

<pre><code class="bash output from the parser ">{'date': '2016-07-12',
 'description': u'COMPANY DESCRIPTION\r\nSAP\u2019s vision is to help the world run better and improve people\u2019s lives.\r\nAs the\u2026',
 'employer': u'SAP SE',
 'jobid': u'106345',
 'location': u'Walldorf, Deutschland',
 'tags': [u'c++', u'python', u'linux', u'git', u'shell'],
 'title': u'C++ (Senior) Developer for SAP HANA',
 'url': u'/jobs/106345/c-plus-plus-senior-developer-for-sap-hana-sap-se'}
</code></pre>

<p>The code for this project can be found in <a href="https://github.com/gyurisc/stackjobs">stackjobs repo on github</a>.</p>

<p>This is exciting. In the next part, I will show how to store the results in a mongoldb database.</p>

<p><strong>Update 2017-06-13</strong>: Fixing small typos, editing and shortening console output.</p>
]]></content>
  </entry>
  
</feed>
