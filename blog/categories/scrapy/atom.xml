<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Scrapy | littlebigtomatoes]]></title>
  <link href="http://www.littlebigtomatoes.com/blog/categories/scrapy/atom.xml" rel="self"/>
  <link href="http://www.littlebigtomatoes.com/"/>
  <updated>2016-11-10T08:14:08+01:00</updated>
  <id>http://www.littlebigtomatoes.com/</id>
  <author>
    <name><![CDATA[Krisztian Gyuris]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Playing With Pandas]]></title>
    <link href="http://www.littlebigtomatoes.com/2016/09/playing-with-pandas/"/>
    <updated>2016-09-26T11:58:11+02:00</updated>
    <id>http://www.littlebigtomatoes.com/2016/09/playing-with-pandas</id>
    <content type="html"><![CDATA[<p>This is the third part of the articles I am writing about my little project I am working on. In part1, I created a web scraper to get the data I needed. In part 2, I added support to save the collected data to a mongodb database. Now in this part, I will look into how to clean up and add new features (columns) to the collected data to make it more suitable for analysis.</p>

<p>My primary motivation here is to learn new technologies as I progress, so my baby steps may not be the state of art in this particular area and all tips and tricks or corrections are welcome.</p>

<p>For this project I am using python and each day I love it more and more. There are some cool libraries for python such as pandas that will be used. There are some col tools such as python notebooks that will be also used.</p>

<!--more-->


<p></p>

<p>For starts, make sure that you have jupiter notebook installed on your machine and then start <a href="http://jupyter.org/">Jupyter Notebooks</a> from the git repo folder.</p>

<pre><code class="bash installing and starting jupiter notebook ">pip install jupiter
cd /path/to/stackjobs
jupiter notebooks
</code></pre>

<p>With this command, we started the iPython (Jupyter) notebooks and a new browser will be opened. Click on the <a href="https://github.com/gyurisc/stackjobs/blob/master/notebooks/Enhancing%20and%20Extending%20data%20with%20Pandas%20.ipynb">Enhancing and Extending data with Pandas</a> notebook to see and run the code that this article with describe. Also the <a href="https://github.com/gyurisc/stackjobs/blob/master/enhance_data_with_pandas.py">enhance_data_with_pandas.py</a> file contains the same code, so it can be run without iptyhon notebook.</p>

<h2>Exploring data with pandas</h2>

<p>First of all we need to import some libraries and make some changes to how the data is printed out.</p>

<p>The changes in the display options e.g display.max_columns and display.max_rows are need in order for me to see the whole data when printed in the notebook and not just a few lines. Feel free to comment it out…</p>

<pre><code class="python importing libraries and setting options and then reading the data">import pandas as pd 
import numpy as np

# Let's change how printing the series works. I need to see all elements in the Series 
# source: http://stackoverflow.com/questions/19124601/is-there-a-way-to-pretty-print-the-entire-pandas-series-dataframe
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

jobs = pd.read_csv('data/stackoverflow_jobs.csv')
</code></pre>

<p>Next, there are three calls to see what kind of data we loaded into our <em>jobs</em> data frame. We print out the columns, the types of the columns and some generic numerical data describing the dataset loaded. In the last one, the most important is the count that is telling us how many rows of data we loaded.</p>

<pre><code class="python ">jobs.columns # just show the columns 
jobs.dtypes
jobs.describe() # show me the row count
</code></pre>

<p>We can also print out the first and last few lines of the data, to have a look what we have using the <em>tail()</em> and <em>head()</em> method calls.</p>

<pre><code class="python display the head and tail of the data">jobs.head()
jobs.tail()
</code></pre>

<h2>Working with Salary</h2>

<p>In our jobs data, we have the <em>salary</em> column that may or may not contains the salary is offered by the company for a particular position. When we have missing values in that column that can cause issues for further processing, so let’s replace the NA values with empty strings.</p>

<pre><code class="python Replacing NA values with empty strings in the salary column">jobs.salary = jobs.salary.fillna('')
</code></pre>

<p>Next, we will extract the information, if the given position offers equity or not and store it in a new column called <em>equity</em>.</p>

<pre><code class="python Extracting equity">jobs['equity'] = jobs['salary'].str.contains('Provides Equity')
</code></pre>

<p>Next, we will move the salary data to a new series object and remove the <em>Provides Equity</em> in order to make it easier to extract the other values. The reason for this is that I would like to leave the other columns as they were originally and do the needed modifications in the new columns or in some temp variables.</p>

<pre><code class="python ">salary = jobs.salary.map(lambda x: x.replace("Provides Equity","").replace("/","").strip())
</code></pre>

<p>When we removed the <em>Provides Equity</em> from the salary information our data is either an empty string or contains the currency information and the low high figure of the salary information. For example:</p>

<pre><code>€50,000 - 71,000
£35,000 - 65,000
$65,000 - 125,000
</code></pre>

<p>This information will be extracted using regular expression and captured in different columns. For the rows with empty values, it is recommended to provide a default value. Empty string for the currency and zero for the low-high figure.</p>

<p>Then we map back the new information to the jobs data frame.</p>

<pre><code class="python extracting and capturing currency and low-high figure. "># salary = jobs.salary
salary = jobs.salary.map(lambda x: x.replace("Provides Equity","").replace("/","").strip())

sal = salary.str.extract('(?P&lt;currency&gt;[^\d]*)(?P&lt;number_low&gt;[\d,]+) - (?P&lt;number_high&gt;[\d,]+$)')

sal.number_low = sal.number_low.fillna(0)
sal.number_high = sal.number_high.fillna(0)
sal.currency = sal.currency.fillna('')

# mapping the new columns back to the original data frame 
jobs['currency'] = sal.currency
jobs['salary_low'] = sal.number_low
jobs['salary_high'] = sal.number_high
</code></pre>

<h2>Fixing up Location</h2>

<p>We will also need better location information, so we can do analysis by countries and cities. For this we need to extract country, state and city out of location column.
But first let&rsquo;s remove the na values from location column. Then use a lambda to split the comma separated location value into individual fields.
Finally, rename the columns to city, location_1 and location_2. The reason for this is that the content of location can be different for different countries.</p>

<pre><code>London,UK
New York,NY
</code></pre>

<p>In the first expression the second member is a country and in the second expression it is a state, so we need to handle these differently.</p>

<pre><code class="python split the location ">jobs.location = jobs.location.fillna('') # sometimes we have nothing in the location field. 

location_split = lambda x: pd.Series([i for i in x.split(',')])
locations = jobs['location'].apply(location_split)

locations.rename(columns={0:'city', 1: 'location_1', 2: 'location_2'},inplace=True)
</code></pre>

<h3>Fixing US locations</h3>

<p>So it seems that US locations are special. They are in the form of city, state, we need this to be in form of city, state, country, so let&rsquo;s fix this first.
If we have a US state in <em>location1 column then put US in </em>location2.</p>

<pre><code class="python Fixing US States ">us_states = ["AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DC", "DE", "FL", "GA", 
          "HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD", 
          "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ", 
          "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC", 
          "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY"]

locations['location_1'] = locations['location_1'].str.strip()
locations.loc[locations['location_1'].isin(us_states),'location_2'] = "US"
</code></pre>

<h3>Filling the state and country columns</h3>

<p>We are now ready to put back the city, state and country information in our original data frame.
Here is the logic, If in a row location_2 is null then location_1 contains the country of that location, if location_2 is not empty then location_2 is going to be the country and location_1 will contain the state.</p>

<pre><code class="python creating city, stat and country column"># if location_2 is null then location_1 column has the country 
# if location_2 is not null then location_2 has the country and location_1 contains the state 
jobs['country'] = np.where(locations['location_2'].isnull(), locations['location_1'], locations['location_2'])
jobs['state'] = np.where(locations['location_2'].notnull(), locations['location_1'], '')

jobs['city'] = locations['city']

# filling na for country 
jobs.country = jobs.country.fillna('')

# stripping spaces from new columns
jobs['city'] = jobs['city'].str.strip()
jobs['country'] = jobs['country'].str.strip()
</code></pre>

<p>Now we can see what countries are posting the most jobs. It seems that the US, Deutschland, Germany and the UK are the top countries. But wait. Aren&rsquo;t Germany and Deutschland are the same country? Let&rsquo;s fix this and some other countries with native names.</p>

<pre><code class="python replacing local country names with their english names"># replacing some of the country names with their english version 
jobs.loc[jobs['country'].str.contains('Deutschland'),'country'] = 'Germany' # Deutschland -&gt; Germany
jobs.loc[jobs['country'].str.contains('Österreich'),'country'] = 'Austria' # Österreich -&gt; Austria
jobs.loc[jobs['country'].str.contains('Suisse'), 'country'] = 'Switzerland' # Suisse -&gt; Switzerland
jobs.loc[jobs['country'].str.contains('Schweiz'), 'country'] = 'Switzerland' # Schweiz -&gt; Switzerland
jobs.loc[jobs['country'].str.contains('Espagne'), 'country'] = 'Spain' # Espagne -&gt; Spain
jobs.loc[jobs['country'].str.contains('République tchèque'), 'country'] = 'Czech Republic' # République tchèque -&gt; Czech Republic
jobs.loc[jobs['country'].str.contains('Niederlande'), 'country'] = 'Netherlands' # Niederlande -&gt; Netherlands

jobs['country'].value_counts().head()
</code></pre>

<p>Now, we have a pretty neat table with data that we can do analytics on. In the next part, I will try to ask questions from  data and create some graphs to find out interesting stuff like:</p>

<ul>
<li>What countries has the most jobs?</li>
<li>What cities has the most jobs?</li>
<li>What technologies are the most popular?</li>
<li>What technologies are paying the most?</li>
</ul>


<p>Coming soon…</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scraping Stackoverflow Careers for Fun and Profit - Part 2]]></title>
    <link href="http://www.littlebigtomatoes.com/2016/08/scraping-stackoverflow-careers-for-fun-and-profit-part-2/"/>
    <updated>2016-08-06T21:20:17+02:00</updated>
    <id>http://www.littlebigtomatoes.com/2016/08/scraping-stackoverflow-careers-for-fun-and-profit-part-2</id>
    <content type="html"><![CDATA[<p>Let’s continue with our project. To summarise what we did in the <a href="http://littlebigtomatoes.com/2016/07/scraping-stackoverflow-careers-for-fun-and-profit/">first part</a>, we wrote a scraper in python using the <a href="http://scrapy.org">scrapy framework</a> that was capable of getting data from the stackoverflow job pages, but nothing else than that.</p>

<p>In this part, we will save the results to mongodb, make sure that we do not have duplicates and learn how to export the data we gathered&hellip;</p>

<!--more-->


<p></p>

<p>The source code for this project can be found in <a href="https://github.com/gyurisc/stackjobs">stackjobs repo</a> on github.</p>

<h3>Installing mongodb</h3>

<p>I need to store the data somewhere and I choose to use mongodb, because of no particular reason :). Let&rsquo;s install it via the <a href="http://brew.sh/">homebrew</a> package manager by using the commands:</p>

<pre><code class="bash">brew update
brew install mongodb
</code></pre>

<p>If you want to download the install package or use some other methods to install, please visit the page <a href="https://docs.mongodb.com/master/tutorial/install-mongodb-on-os-x/">Install MongoDB Community Edition on OS X</a> and choose the method that suits you.</p>

<p>Then create the directory structure for mongodb in a suitable location and start the server</p>

<pre><code class="bash ">mkdir mongodb/data/db
mongod --rest --dbpath mongodb/ 
</code></pre>

<p>You can confirm that the mongodb service is up and running by visiting <a href="http://localhost:28017">http://localhost:28017</a>.</p>

<h4>How to configure mongo to run as a service?</h4>

<p>Because, I installed mongodb using brew I can also start, stop and list mongodb as a service on macOS with the following command:</p>

<pre><code class="bash Starting and stopping mongodb servce using brew.">brew services start mongodb
brew services stop mongodb
brew services list
</code></pre>

<p>My only problem is with this approach is that I do not know how can I enable the &ndash;rest interface in this case, but navigating to <a href="http://localhost:27017">http://localhost:27017</a> I get the standard mongodb warning, meaning that my database engine is running and that is good enough for now.</p>

<p>In order, to communicate with mongodb in python, you will need to install pymongo module using pip:</p>

<pre><code class="bash Installing pymongo">pip install pymongo
</code></pre>

<h3>Saving the items in db</h3>

<p>To be able to save the items gathered you will need to add some configuration to your project and also add some code to your pipelines.py.</p>

<h4>Setting up a Pipeline</h4>

<p>First, let’s see how the pipeline works. We will define a new class called MongoDBPipeline. This class has a constructor and a process_item method that will be called whenever a new item is created.</p>

<p>We will be using the <strong>process_item</strong> to check the validity of the item first, then it will be checked against a list called <strong>ids_seen</strong>. This list contains all the items that we already processed. We will ensure this way that there will be  no duplicated items in our database.</p>

<p>If the item is valid and it is not in our list of previous items then we ca add it to our mongo database.</p>

<pre><code class="python Validating and adding the item to our mongodb">    def process_item(self, item, spider):

        valid = True
        for data in item:
            if not data:
                valid = False
                raise DropItem("Missing {0}!".format(data))

        if valid and item['jobid'] not in self.ids_seen:
            self.collection.insert(dict(item))
            self.ids_seen.add(item['jobid'])            
            log.msg("Job added to MongoDB database!",
                    level=log.DEBUG, spider=spider)
        else:
            raise DropItem("Job id {0} already exists!".format(item['jobid']))    

        return item        
</code></pre>

<p>Now, we need to check out the constructor for this class. Here we initialise the connection and fill the <strong>ids_seen</strong> list with the identifiers that we already have in the database.</p>

<pre><code class="python Initialising database connection and fill up our id list.">import pymongo
import sys

from scrapy.conf import settings
from scrapy.exceptions import DropItem
from scrapy import log

class MongoDBPipeline(object):
    def __init__(self):
        connection = pymongo.MongoClient(
            settings['MONGODB_SERVER'],
            settings['MONGODB_PORT']
        )
        db = connection[settings['MONGODB_DB']]
        self.collection = db[settings['MONGODB_COLLECTION']]
        self.ids_seen = set()
        for job in self.collection.find():
            self.ids_seen.add(job['jobid'])

        print "Number of jobs in database"
        print len(self.ids_seen)
</code></pre>

<h4>Configuring the mongodb connection.</h4>

<p>We need to do a little more work in order to be able to connect to the database and to register the newly added MongoDBPipeline, so it is actually called on the new items. In the settings.py, we need to added either add or modify the following line, so it looks like this:</p>

<pre><code class="python ">ITEM_PIPELINES = {'stackjobs.pipelines.MongoDBPipeline' : 300, }

MONGODB_SERVER = "localhost"
MONGODB_PORT = 27017
MONGODB_DB = "stackoverflow"
MONGODB_COLLECTION = "jobs"
</code></pre>

<p>With the first line, we register our pipeline class on the system and assign a priority. With this mechanism, we can specify    in what we order we want the pipelines to be executed, if we would have more than one pipelines.</p>

<p>The four lines after that are specifying how to connect to the database, such as the name of the server and server port, database name and collection name (table name).</p>

<p>With these changes we can now run our parser and all items scraped will be stored in our mongo database.</p>

<h3>Exporting data from mongo</h3>

<p>What to do next? Maybe, we want to export the data to a file, so we can do further analysis and cleaning up on it. For this, I used the mongoexport command with the following parameters to create a csv file</p>

<pre><code class="bash Exporting data to csv file.">mongoexport --db stackoverflow --collection jobs --type csv --fields jobid,title,employer,location,salary,description,tags,url,date --out stackoverflow_jobs.csv
</code></pre>

<p>It is also possible to export the data in son format, if needed.  For that, you can use the following command:</p>

<pre><code class="bash Exporting data in json format. ">mongoexport --db stackoverflow --collection jobs --type json --fields jobid,title,employer,location,salary,description,tags,url,date --out stackoverflow_jobs.json
</code></pre>

<h3>Scheduling our job</h3>

<p>The last thing we need to do is to schedule our scraper to run on my mac. This can be achieved using crontab. I will schedule my scraper to run in every 6 hours as I do not think that running it more often would make sense.</p>

<p>First, you will need to start the nano editor to edit your crontab file:</p>

<pre><code class="bash Launching nano to edit crontab.">env EDITOR=nano crontab -e
</code></pre>

<p>Now enter the following to run our scraper every 6 hours. Use Ctrl-0 and Ctrl-X to exit the editor and save.</p>

<pre><code class="bash Enter this line to your crontab file. ">0 */6 * * *  cd ~/dev/scrapers/stackjobs &amp;&amp; /usr/local/bin/scrapy crawl Stackjob
</code></pre>

<p>To list the existing crontab jobs, we can use the following command:</p>

<pre><code class="bash Listing existing crontab jobs ">crontab -l 
</code></pre>

<h3>Summary</h3>

<p>In this part, we have installed mongodb on our machine and added logic and configuration to the scrapy project, so we can save all items that has been found to the database. Also, We have added a way to export the data into CSV or JSON file format from the mongo database. Finally, added this script to the crontab job and scheduled to run it every 2 hours.</p>

<p>In the next part, I will spend a little time on the exported data and use pandas to transform the exported data into a a better consumable format.</p>

<h3>Additional Information:</h3>

<ul>
<li><a href="https://ole.michelsen.dk/blog/schedule-jobs-with-crontab-on-mac-osx.html">Schedule jobs with crontab on Mac OS X</a></li>
<li><a href="http://alvinalexander.com/linux/unix-linux-crontab-every-minute-hour-day-syntax">Unix &amp; Linux crontab every examples</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scraping Stackoverflow Careers for Fun and Profit]]></title>
    <link href="http://www.littlebigtomatoes.com/2016/07/scraping-stackoverflow-careers-for-fun-and-profit/"/>
    <updated>2016-07-12T07:55:13+02:00</updated>
    <id>http://www.littlebigtomatoes.com/2016/07/scraping-stackoverflow-careers-for-fun-and-profit</id>
    <content type="html"><![CDATA[<p>When you want to learn something new the best way to do is to come up with a problem that can be useful to you or maybe to others and then solve it. Not so long ago I decided that I want to start learning something data related. So I came up with the idea that I would create a solution that gathers the data from <a href="http://stackoverflow.com/jobs">Stackoverflow Jobs</a>. Do some manipulation on it  and then present it in a nice format. This is the first step of the journey.</p>

<p>To present something, I need data. To get the data, I need to write a script that gathers data from the web. I decided to use python and <a href="http://scrapy.org">Scrapy framework</a>.</p>

<!--more-->


<p></p>

<h3>Gearing up…</h3>

<p>First, some disclaimers. I am new to python and to scrapy as well. I had zero knowledge any of these topics before. I am learning this stuff as I write these posts. Any constructive and helpful comments are more than welcome.</p>

<p>The code for this project can be found in <a href="https://github.com/gyurisc/stackjobs">stackjobs repo on github</a>.</p>

<p>After installing Python and <a href="http://scrapy.org">Scrapy</a>, the first thing is to create the web-scraper project by typing the following  command(s) in your console:</p>

<pre><code class="bash">        scrapy startproject stackjobs
        cd stackjobs
        scrapy genspider Stackjob stackoverflow.com
</code></pre>

<p>With these commands the following has been achieved. First we created a new project called stackjobs. The script created a folder structure with files in it. Then we entered the folder and created a spider called Stackjob using the genspider command. This created a spider that can crawl stackoverflow.com.</p>

<p>Looking at the spiders folder, we can see the source file for Stackjob spider:</p>

<pre><code class="python spider created ">        # -*- coding: utf-8 -*-
        import scrapy


        class StackjobSpider(scrapy.Spider):
            name = "Stackjob"
            allowed_domains = ["stackoverflow.com"]
            start_urls = (
          'http://www.stackoverflow.com/',
            )

            def parse(self, response):
                pass
</code></pre>

<p>You can run the spider this by executing the following command:</p>

<pre><code class="bash running the spider ">scrapy crawl Stackjob
</code></pre>

<p>This outputs a bunch of stuff that we are not interested in really.</p>

<pre><code class="bash bunch of stuff we are not really interested in">2016-07-11 22:22:30 [scrapy] INFO: Scrapy 1.1.0 started (bot: stackjobs)
2016-07-11 22:22:30 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'stackjobs.spiders', 'SPIDER_MODULES': ['stackjobs.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'stackjobs'}
…
2016-07-11 22:22:30 [scrapy] INFO: Enabled item pipelines:
[]
2016-07-11 22:22:30 [scrapy] INFO: Spider opened
2016-07-11 22:22:30 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-07-11 22:22:30 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2016-07-11 22:22:30 [scrapy] DEBUG: Redirecting (301) to &lt;GET http://stackoverflow.com/robots.txt&gt; from &lt;GET http://www.stackoverflow.com/robots.txt&gt;
2016-07-11 22:22:30 [scrapy] DEBUG: Crawled (200) &lt;GET http://stackoverflow.com/robots.txt&gt; (referer: None)
2016-07-11 22:22:31 [scrapy] DEBUG: Redirecting (301) to &lt;GET http://stackoverflow.com/&gt; from &lt;GET http://www.stackoverflow.com/&gt;
2016-07-11 22:22:31 [scrapy] DEBUG: Crawled (200) &lt;GET http://stackoverflow.com/robots.txt&gt; (referer: None)
2016-07-11 22:22:31 [scrapy] DEBUG: Crawled (200) &lt;GET http://stackoverflow.com/&gt; (referer: None)
2016-07-11 22:22:31 [scrapy] INFO: Closing spider (finished)
2016-07-11 22:22:31 [scrapy] INFO: Spider closed (finished)
</code></pre>

<p>This basically tells us that our spider ran and downloaded zero pages exactly. We are interested in the Stackoverflow job listing, so let’s modify the Stackjob spider by adding start urls that are relevant for us:</p>

<pre><code class="python adding start urls ">class StackjobSpider(scrapy.Spider):
    name = "Stackjob"
    allowed_domains = ["stackoverflow.com"]
    start_urls = (
        'https://stackoverflow.com/jobs?sort=p',
        'https://stackoverflow.com/jobs?sort=p&amp;pg=2',
        'https://stackoverflow.com/jobs?sort=p&amp;pg=3',
        'https://stackoverflow.com/jobs?sort=p&amp;pg=4'
    )

    def parse(self, response):
        pass
</code></pre>

<p>When running the spider again we can find that still nothing really happens. The pages are downloaded, but we do not process the information on the pages. In order to process the information, let’s modify the parse method in our spider.</p>

<p>First we need to add a few imports:</p>

<pre><code class="python">import scrapy
import datetime

class StackjobSpider(scrapy.Spider):
    name = "Stackjob"
    allowed_domains = ["stackoverflow.com"]
    start_urls = (
        'https://stackoverflow.com/jobs?sort=p',
        'https://stackoverflow.com/jobs?sort=p&amp;pg=2',
        'https://stackoverflow.com/jobs?sort=p&amp;pg=3'
    )
</code></pre>

<p>Then add some code to the <em>parse</em> method. This code will loop through all the jobs on the page found by our path expression and will create a <em>StackjobItem</em> for each job found. Then it will add today’s date to the newly created item.</p>

<pre><code class="python adding parsing logic ">    def parse(self, response):
        jobs = scrapy.Selector(response).xpath('//div[contains(@class, "-item") and contains(@class, "-job")]')
        results = [] 

        t = datetime.date.today()

        for job in jobs: 
            item = StackjobItem()  
                        item['date'] = t.strftime('%Y-%m-%d')       

                        # actual parsing will come here 

            results.append(item)

        return results 
</code></pre>

<p>When running this code, you will receive a couple of messages complaining about StackjobItem not being defined. This is expected as we still need to define this class. Open up <em>items.py</em> and rename the class <em>StackjobsItem</em> class to <em>StackjobItem</em>. This is the class what we will use to store the extracted data for each job. For this you will need to define a few fields. The final class will look like this:</p>

<pre><code class="python StackjobItem with fields">
import scrapy
from scrapy.item import Item, Field

class StackjobItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    title = Field()
    url = Field()
    jobid = Field()
    employer = Field()
    description = Field()    
    location = Field()
    equity = Field()
    salary = Field()
    tags = Field()    
    date = Field()
    pass
</code></pre>

<p>Now, head back to <em>Stackjob.py</em> and import the item class.</p>

<pre><code class="python ">from stackjobs.items import StackjobItem
</code></pre>

<p>When running the spider, you should see something like this in the output:</p>

<pre><code class="bash ">2016-07-12 07:07:17 [scrapy] DEBUG: Scraped from &lt;200 https://stackoverflow.com/jobs?sort=p&amp;pg=3&gt;
{'date': '2016-07-12'}
2016-07-12 07:07:17 [scrapy] DEBUG: Scraped from &lt;200 https://stackoverflow.com/jobs?sort=p&amp;pg=3&gt;
{'date': '2016-07-12'}
2016-07-12 07:07:17 [scrapy] DEBUG: Scraped from &lt;200 https://stackoverflow.com/jobs?sort=p&amp;pg=3&gt;
{'date': '2016-07-12'}
2016-07-12 07:07:17 [scrapy] DEBUG: Scraped from &lt;200 https://stackoverflow.com/jobs?sort=p&amp;pg=3&gt;
{'date': '2016-07-12'}
2016-07-12 07:07:17 [scrapy] DEBUG: Scraped from &lt;200 https://stackoverflow.com/jobs?sort=p&amp;pg=3&gt;
{'date': '2016-07-12'}
</code></pre>

<p>Now, this tells us that the items are being generated and each item has one field only and that is the <em>date</em> field. This is not really useful, so let’s add more information.</p>

<h3>To the actual scraping</h3>

<p>For getting information out of the items found on the page, we will be using xpath expressions. Earlier, we added one to our parse method. This selector is responsible to find all the jobs on our page. It basically tells scrapy to return all elements that are a div and has both a <em>-job</em> and <em>-item</em> css class.</p>

<pre><code class="python ">jobs = Selector(response).xpath('//div[contains(@class, "-item") and contains(@class, "-job")]')
</code></pre>

<p>Next, we will enumerate through the collection returned in the <em>jobs</em> variable and extract some more information from each item and store it.</p>

<p>In the <em>parse</em> method of our spider, just below the date field add the following lines:</p>

<pre><code class="python xpaths and css selectors">item['tags'] = job.xpath('div[contains(@class,"tags")]/p/a/text()').extract()
item['description'] = job.xpath('p[contains(@class,"text") and contains(@class, "description")]/text()').extract()[0]
item['location'] = job.xpath('ul[contains(@class, "metadata") and contains(@class, "primary")]/li[contains(@class, "location")]/text()').extract()[0].strip()
item['employer'] = job.xpath('ul[contains(@class, "metadata") and contains(@class, "primary")]/li[contains(@class, "employer")]/text()').extract()[0].strip()
item['jobid'] = job.xpath('@data-jobid').extract()[0]
item['title'] = job.xpath('div[contains(@class,"-title")]/h1/a/text()').extract()[0]
item['url'] = job.xpath('div[contains(@class,"-title")]/h1/a/@href').extract()[0]
</code></pre>

<p>Here, we have an xpath expression for each field that we are interested in. For coming up with the xpath expression I was using Google Chrome and its wonderful <a href="https://www.codeschool.com/courses/discover-devtools">DevTools</a>. You can bring this up by opening a web-page and right-clicking on any element and select <em>Inspect</em> from the menu.</p>

<p>The other stuff we need to come up with the expressions is the knowledge of css and lot’s of trial and error.</p>

<h3>XPath and CSS munging</h3>

<p>The other stuff we need to come up with the expressions is the knowledge of css and lot’s of trial and error. Let’s see a few expressions:</p>

<pre><code class="python title and url ">item['title'] = job.xpath('div[contains(@class,"-title")]/h1/a/text()').extract()[0]
item['url'] = job.xpath('div[contains(@class,"-title")]/h1/a/@href').extract()[0]
</code></pre>

<p>Here, we are searching with a div that has the <em>title</em> as a css class and then we need the link <em>a</em> under the <em>h1</em> heading. The text from this link will be store in the <em>title</em> field and the actual link in the <em>url</em> field.</p>

<pre><code class="python extracting tags">item['tags'] = job.xpath('div[contains(@class,"tags")]/p/a/text()').extract()
</code></pre>

<p>Here, we are looking for a div with a class <em>tags</em> and all the text from the links underneath the paragraph.</p>

<pre><code class="python extracting the employer"> item['employer'] = job.xpath('ul[contains(@class, "metadata") and contains(@class, "primary")]/li[contains(@class, "employer")]/text()').extract()[0].strip()
</code></pre>

<p>Here, we are looking for an unordered list <em>ul</em> with both a class <em>metadata</em> and <em>primary</em> and underneath we are looking for a list item with <em>employer</em> class. We then extract the first item from the returned list and call <em>strip()</em> to get rid of the whitespace characters.</p>

<p>There is one last field that needs to be extracted called <em>salary</em>. This is a bit tricky, because it is not always present.</p>

<p>First, we need to check, if we have an html span tag with class <em>salary</em> our title, if we have then we just extract it and then strip out of the white spaces from the result.</p>

<pre><code class="python checking if salary is present and extracting it ">if job.xpath('div[contains(@class,"-title”)]/span[contains(@class,"salary")]/text()').extract():
    item['salary'] = job.xpath('div[contains(@class,"-title")]/span[contains(@class,"salary")]/text()').extract()[0].strip()
</code></pre>

<p>Here is how the full parse method should look like:</p>

<pre><code class="python final parse method ">
    def parse(self, response):
        jobs = Selector(response).xpath('//div[contains(@class, "-item") and contains(@class, "-job")]')
        results = [] 

        t = datetime.date.today()

        for job in jobs: 
            item = StackjobItem()    
            item['date'] = t.strftime('%Y-%m-%d')                       
            item['tags'] = job.xpath('div[contains(@class,"tags")]/p/a/text()').extract()
            item['description'] = job.xpath('p[contains(@class,"text") and contains(@class, "description")]/text()').extract()[0]
            item['location'] = job.xpath('ul[contains(@class, "metadata") and contains(@class, "primary")]/li[contains(@class, "location")]/text()').extract()[0].strip()
            item['employer'] = job.xpath('ul[contains(@class, "metadata") and contains(@class, "primary")]/li[contains(@class, "employer")]/text()').extract()[0].strip()
            item['jobid'] = job.xpath('@data-jobid').extract()[0]
            item['title'] = job.xpath('div[contains(@class,"-title")]/h1/a/text()').extract()[0]
            item['url'] = job.xpath('div[contains(@class,"-title")]/h1/a/@href').extract()[0]

            if job.xpath('div[contains(@class,"-title")]/span[contains(@class,"salary")]/text()').extract():
                item['salary'] = job.xpath('div[contains(@class,"-title")]/span[contains(@class,"salary")]/text()').extract()[0].strip()

            results.append(item)
            # yield item

        return results 
</code></pre>

<p>After running the parser, we should see the extra fields being added to our item.</p>

<pre><code class="bash output from the parser ">{'date': '2016-07-12',
 'description': u'COMPANY DESCRIPTION\r\nSAP\u2019s vision is to help the world run better and improve people\u2019s lives.\r\nAs the\u2026',
 'employer': u'SAP SE',
 'jobid': u'106345',
 'location': u'Walldorf, Deutschland',
 'tags': [u'c++', u'python', u'linux', u'git', u'shell'],
 'title': u'C++ (Senior) Developer for SAP HANA',
 'url': u'/jobs/106345/c-plus-plus-senior-developer-for-sap-hana-sap-se'}
</code></pre>

<p>The code for this project can be found in <a href="https://github.com/gyurisc/stackjobs">stackjobs repo on github</a>.</p>

<p>This is exciting. In the next part, I will show how to store the results in a mongoldb database.</p>

<p><strong>Update 2017-06-13</strong>: Fixing small typos, editing and shortening console output.</p>
]]></content>
  </entry>
  
</feed>
